{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AcronymFeedback(BaseModel):\n",
    "    relevance: str = str()\n",
    "    relevance_score: int = 0\n",
    "    pronunciation: str = str()\n",
    "    pronunciation_score: int = 0\n",
    "    spelling: str = str()\n",
    "    spelling_score: int = 0\n",
    "    familiarity: str = str()\n",
    "    familiarity_score: int = 0\n",
    "    total_score: int = 0\n",
    "\n",
    "\n",
    "class AcronymIteration(BaseModel):\n",
    "    n: int = 0\n",
    "    acronym: str = str()\n",
    "    feedback: AcronymFeedback = AcronymFeedback()\n",
    "\n",
    "\n",
    "class AcronymGenOutput(BaseModel):\n",
    "    title: str\n",
    "    best: AcronymIteration\n",
    "    n: int\n",
    "    iterations: list[AcronymIteration]\n",
    "\n",
    "\n",
    "class AcronymEvalOutput(BaseModel):\n",
    "    title: str\n",
    "    acronym_a: str\n",
    "    feedback_a: AcronymFeedback\n",
    "    acronym_b: str\n",
    "    feedback_b: AcronymFeedback\n",
    "\n",
    "\n",
    "class DialogFeedback(BaseModel):\n",
    "    consistency: str = str()\n",
    "    consistency_score: int = 0\n",
    "    understand: str = str()\n",
    "    understand_score: int = 0\n",
    "    sustain: str = str()\n",
    "    sustain_score: int = 0\n",
    "    total_score: int = 0\n",
    "\n",
    "\n",
    "class DialogIteration(BaseModel):\n",
    "    n: int = 0\n",
    "    response: str = str()\n",
    "    feedback: DialogFeedback = DialogFeedback()\n",
    "\n",
    "\n",
    "class DialogGenOutput(BaseModel):\n",
    "    dialog: list[str]\n",
    "    best: DialogIteration\n",
    "    n: int\n",
    "    iterations: list[DialogIteration]\n",
    "\n",
    "\n",
    "class DialogEvalOutput(BaseModel):\n",
    "    dialog: list[str]\n",
    "    response_a: str\n",
    "    feedback_a: DialogFeedback\n",
    "    response_b: str\n",
    "    feedback_b: DialogFeedback\n",
    "\n",
    "\n",
    "class SentenceFeedback(BaseModel):\n",
    "    inclusion: str = str()\n",
    "    inclusion_score: int = 0\n",
    "    logical: str = str()\n",
    "    logical_score: int = 0\n",
    "    total_score: int = 0\n",
    "\n",
    "\n",
    "class SentenceIteration(BaseModel):\n",
    "    n: int = 0\n",
    "    sentence: str = str()\n",
    "    feedback: SentenceFeedback = SentenceFeedback()\n",
    "\n",
    "\n",
    "class SentenceGenOutput(BaseModel):\n",
    "    concepts: list[str]\n",
    "    best: SentenceIteration\n",
    "    n: int\n",
    "    iterations: list[SentenceIteration]\n",
    "\n",
    "\n",
    "class SentenceEvalOutput(BaseModel):\n",
    "    concepts: list[str]\n",
    "    sentence_a: str\n",
    "    feedback_a: SentenceFeedback\n",
    "    sentence_b: str\n",
    "    feedback_b: SentenceFeedback\n",
    "\n",
    "\n",
    "class SentimentFeedback(BaseModel):\n",
    "    effective: str = str()\n",
    "    effective_score: int = 0\n",
    "    logical: str = str()\n",
    "    logical_score: int = 0\n",
    "    total_score: int = 0\n",
    "\n",
    "\n",
    "class SentimentIteration(BaseModel):\n",
    "    n: int = 0\n",
    "    reversed_review: str = str()\n",
    "    feedback: SentimentFeedback = SentimentFeedback()\n",
    "\n",
    "\n",
    "class SentimentGenOutput(BaseModel):\n",
    "    review: str\n",
    "    best: SentimentIteration\n",
    "    n: int\n",
    "    iterations: list[SentimentIteration]\n",
    "\n",
    "\n",
    "class SentimentEvalOutput(BaseModel):\n",
    "    review: str\n",
    "    reversed_review_a: str\n",
    "    feedback_a: SentimentFeedback\n",
    "    reversed_review_b: str\n",
    "    feedback_b: SentimentFeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"acronym\": (AcronymGenOutput, AcronymEvalOutput, \"acronym\", 40),\n",
    "    \"dialog\": (DialogGenOutput, DialogEvalOutput, \"response\", 30),\n",
    "    \"sentence\": (SentenceGenOutput, SentenceEvalOutput, \"sentence\", 20),\n",
    "    \"sentiment\": (SentimentGenOutput, SentimentEvalOutput, \"reversed_review\", 20),\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"llama-2-7b\": (\"llama-2-7b-f.jsonl\", \"llama-2-7b-f-e.jsonl\"),\n",
    "    \"mistral-7b\": (\"mistral-7b-v0.1-f.jsonl\", \"mistral-7b-v0.1-f-e.jsonl\"),\n",
    "    \"openchat-3.5\": (\"openchat-3.5-f.jsonl\", \"openchat-3.5-f-e.jsonl\"),\n",
    "    \"gpt-3.5\": (\"gpt-3.5-f.jsonl\", \"gpt-3.5-f-e.jsonl\"),\n",
    "}\n",
    "\n",
    "feedbacks = {\n",
    "    \"full feedback\": {\"llama-2-7b\": \"llama-2-7b-f-e.jsonl\", \"mistral-7b\": \"mistral-7b-v0.1-f-e.jsonl\", \"openchat-3.5\": \"openchat-3.5-f-e.jsonl\", \"gpt-3.5\": \"gpt-3.5-f-e.jsonl\"},\n",
    "    \"generic feedback\": {\"llama-2-7b\": \"llama-2-7b-g-e.jsonl\", \"mistral-7b\": \"mistral-7b-v0.1-g-e.jsonl\", \"openchat-3.5\": \"openchat-3.5-g-e.jsonl\", \"gpt-3.5\": \"gpt-3.5-g-e.jsonl\"},\n",
    "    \"no feedback\": {\"llama-2-7b\": \"llama-2-7b-m-e.jsonl\", \"mistral-7b\": \"mistral-7b-v0.1-m-e.jsonl\", \"openchat-3.5\": \"openchat-3.5-m-e.jsonl\", \"gpt-3.5\": \"gpt-3.5-m-e.jsonl\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def sum_up_scores_by_models(task, model):\n",
    "    count = 0\n",
    "    base_total = 0\n",
    "    refine_total = 0\n",
    "    with open(Path(\"outputs\") / task / models[model][1], mode=\"r\") as f:\n",
    "        for line in f:\n",
    "            obj = tasks[task][1].model_validate_json(line)\n",
    "            count += 1\n",
    "            base_total += obj.feedback_a.total_score\n",
    "            refine_total += obj.feedback_b.total_score\n",
    "    base_percent = base_total / count * 100 / tasks[task][3]\n",
    "    refine_percent = refine_total / count * 100 / tasks[task][3]\n",
    "    return base_percent, refine_percent\n",
    "\n",
    "\n",
    "def create_score_df_by_models():\n",
    "    data = list()\n",
    "    for task in tasks:\n",
    "        record = dict()\n",
    "        for model in models:\n",
    "            base_percent, refine_percent = sum_up_scores_by_models(task, model)\n",
    "            record[model + \" (base)\"] = f\"{round(base_percent, 1)}\"\n",
    "            record[model + \" (self-refine)\"] = f\"{round(refine_percent, 1)} ({round(refine_percent - base_percent, 1)})\"\n",
    "        data.append(record)\n",
    "    df = pd.DataFrame(data, index=list(tasks.keys()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def sum_up_scores_by_feedbacks(model, feedback):\n",
    "    task = \"sentiment\"\n",
    "    count = 0\n",
    "    base_total = 0\n",
    "    refine_total = 0\n",
    "    with open(Path(\"outputs\") / task / feedbacks[feedback][model], mode=\"r\") as f:\n",
    "        for line in f:\n",
    "            obj = tasks[task][1].model_validate_json(line)\n",
    "            count += 1\n",
    "            base_total += obj.feedback_a.total_score\n",
    "            refine_total += obj.feedback_b.total_score\n",
    "    base_percent = base_total / count * 100 / tasks[task][3]\n",
    "    refine_percent = refine_total / count * 100 / tasks[task][3]\n",
    "    return base_percent, refine_percent\n",
    "\n",
    "\n",
    "def create_score_df_by_feedbacks():\n",
    "    data = list()\n",
    "    for model in models:\n",
    "        record = dict()\n",
    "        for feedback in feedbacks:\n",
    "            base_percent, refine_percent = sum_up_scores_by_feedbacks(model, feedback)\n",
    "            record[feedback + \" (base)\"] = f\"{round(base_percent, 1)}\"\n",
    "            record[feedback + \" (self-refine)\"] = f\"{round(refine_percent, 1)} ({round(refine_percent - base_percent, 1)})\"\n",
    "        data.append(record)\n",
    "    df = pd.DataFrame(data, index=list(models.keys()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def sum_up_refine_count_by_models(task, model):\n",
    "    count = 0\n",
    "    refine_count = 0\n",
    "    with open(Path(\"outputs\") / task / models[model][0], mode=\"r\") as f:\n",
    "        for line in f:\n",
    "            obj = tasks[task][0].model_validate_json(line)\n",
    "            count += 1\n",
    "            if obj.best.n != 1 and getattr(obj.iterations[0], tasks[task][2]) != getattr(obj.best, tasks[task][2]):\n",
    "                refine_count += 1\n",
    "    refine_percent = refine_count / count * 100\n",
    "    return refine_percent\n",
    "\n",
    "\n",
    "def create_refine_count_df_by_models():\n",
    "    data = list()\n",
    "    for task in tasks:\n",
    "        record = dict()\n",
    "        for model in models:\n",
    "            refine_percent = sum_up_refine_count_by_models(task, model)\n",
    "            record[model] = f\"{round(refine_percent, 1)}\"\n",
    "        data.append(record)\n",
    "    df = pd.DataFrame(data, index=list(tasks.keys()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_score_df_by_models()\n",
    "df.to_csv(\"score_by_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_score_df_by_feedbacks()\n",
    "df.to_csv(\"score_by_feedbacks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_refine_count_df_by_models()\n",
    "df.to_csv(\"refine_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_evaluation(task, model):\n",
    "    count = 0\n",
    "    with (\n",
    "        open(Path(\"outputs\") / task / models[model][0], mode=\"r\") as g_f,\n",
    "        open(Path(\"outputs\") / task / models[model][1], mode=\"r\") as e_f,\n",
    "        open(Path(\"outputs\") / task / (models[model][1] + \".fix\"), mode=\"w\") as w_f,\n",
    "    ):\n",
    "        for i, (g_l, e_l) in enumerate(zip(g_f, e_f)):\n",
    "            g_obj = tasks[task][0].model_validate_json(g_l)\n",
    "            e_obj = tasks[task][1].model_validate_json(e_l)\n",
    "            if g_obj.best.n != 1 and getattr(g_obj.iterations[0], tasks[task][2]) == getattr(g_obj.best, tasks[task][2]):\n",
    "                print(f\"Found conflict in {task}/{model} ({count + 1}). Line: {i + 1}.\", flush=True)\n",
    "                print(f'A: {getattr(e_obj, tasks[task][2] + \"_a\")}', flush=True)\n",
    "                print(f'B: {getattr(e_obj, tasks[task][2] + \"_b\")}', flush=True)\n",
    "                print(f\"Score A: {e_obj.feedback_a.total_score}\", flush=True)\n",
    "                print(f\"Score B: {e_obj.feedback_b.total_score}\", flush=True)\n",
    "                count += 1\n",
    "                e_obj.feedback_b = e_obj.feedback_a\n",
    "            print(e_obj.model_dump_json(), file=w_f, flush=True)\n",
    "\n",
    "\n",
    "def fix_all_evaluation():\n",
    "    for task in tasks:\n",
    "        for model in models:\n",
    "            fix_evaluation(task, model)\n",
    "\n",
    "\n",
    "def detect_evaluation(task, model):\n",
    "    count = 0\n",
    "    with (open(Path(\"outputs\") / task / models[model][1], mode=\"r\") as f,):\n",
    "        for i, line in enumerate(f):\n",
    "            obj = tasks[task][1].model_validate_json(line)\n",
    "            if getattr(obj, tasks[task][2] + \"_a\") == getattr(obj, tasks[task][2] + \"_b\") and obj.feedback_a != obj.feedback_b:\n",
    "                print(f\"Found conflict in {task}/{model} ({count + 1}). Line: {i + 1}.\", flush=True)\n",
    "                print(f'A: {getattr(obj, tasks[task][2] + \"_a\")}', flush=True)\n",
    "                print(f'B: {getattr(obj, tasks[task][2] + \"_b\")}', flush=True)\n",
    "                print(f\"Score A: {obj.feedback_a.total_score}\", flush=True)\n",
    "                print(f\"Score B: {obj.feedback_b.total_score}\", flush=True)\n",
    "                count += 1\n",
    "\n",
    "\n",
    "def detect_all_evaluation():\n",
    "    for task in tasks:\n",
    "        for model in models:\n",
    "            detect_evaluation(task, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_all_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-refine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
